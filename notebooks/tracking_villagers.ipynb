{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "import pyautogui\n",
    "from PIL import Image \n",
    "from Xlib import display\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from mss.linux import MSS as mss\n",
    "import mss.tools\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import transforms, utils\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "import segmentation_models_pytorch as smp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENCODER = 'resnet18'\n",
    "ENCODER_WEIGHTS = 'imagenet'\n",
    "CLASSES = [\"villager\"]\n",
    "ACTIVATION = 'sigmoid' # could be None for logits or 'softmax2d' for multicalss segmentation\n",
    "DEVICE = 'cuda'\n",
    "in_channels = 3\n",
    "\n",
    "# create segmentation model with pretrained encoder\n",
    "model = smp.FPN(\n",
    "    encoder_name=ENCODER, \n",
    "    encoder_weights=ENCODER_WEIGHTS, \n",
    "    classes=len(CLASSES), \n",
    "    activation=ACTIVATION,\n",
    "    in_channels = in_channels \n",
    ")\n",
    "\n",
    "preprocessing_fn = smp.encoders.get_preprocessing_fn(ENCODER, ENCODER_WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"models/model.pth\", map_location = \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_transforms = transforms.Compose([ \n",
    "                                                                    \n",
    "                                    transforms.ToPILImage(),\n",
    "                                    transforms.Resize((256,256), interpolation = Image.NEAREST),\n",
    "                                    ToTensor(),\n",
    "\n",
    "                                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def screen_record_efficient(top = 225, left = 0, width = 800, height = 100):\n",
    "  \n",
    "    mon = {\"top\": top, \"left\": left, \"width\": width, \"height\": height}\n",
    "    sct = mss.mss()\n",
    "    img = np.array(sct.grab(mon))\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGBA2RGB)\n",
    "\n",
    "    sct.close()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_blobs_return_vertices(thresh_image, minimum_size = 50, padding = 20):\n",
    "\n",
    "    rect_vertices = []\n",
    "    contours, hierarchy = cv2.findContours(thresh_image, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "    for contour in contours:\n",
    "        (x, y, w, h) = cv2.boundingRect(contour)\n",
    "        if 0 in (x,y,w,h): ## skip invali stuff \n",
    "            continue\n",
    "        if w < minimum_size or h < minimum_size :  # skip if too small to be a worm\n",
    "            continue\n",
    "        w += padding \n",
    "        h+= padding \n",
    "        rect_vertices.append([(x - padding , y - padding ), (x+w, y+h)])\n",
    "    return rect_vertices\n",
    "\n",
    "def draw_rectangle(image, vertex1, vertex2):\n",
    "    image = cv2.rectangle(image,\n",
    "                    vertex1,\n",
    "                    vertex2,\n",
    "                    color=(0, 255, 0), thickness=3)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "dual_monitor_top = 1080 + 35\n",
    "single_monitor_top = 35 + 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    foo = screen_record_efficient(top = single_monitor_top,width = 800, height = 510)\n",
    "    \n",
    "    inp = mini_transforms(foo).unsqueeze(0)\n",
    "    pred = model(inp).detach().numpy()[0][0]\n",
    "#     print(pred.shape)\n",
    "    \n",
    "#     cv2.imshow(\"im\", cv2.resize(foo, (256,256)))    \n",
    "    \n",
    "    cv2.imshow(\"im\",pred)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        break\n",
    "        \n",
    "cv2.destroyAllWindows()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
